{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKRprnMBC0wJ",
        "outputId": "98d21573-b795-4194-961e-c54817b6f3c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import requests\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# Download 'punkt_tab' data package\n",
        "nltk.download('punkt_tab') # Download the required data package. This data is not installed by default\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "url = \"https://huggingface.co/datasets/sleeping-ai/TEKGEN-Wiki\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "# Save the data to a file\n",
        "with open(\"dataset.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(data)"
      ],
      "metadata": {
        "id": "ueTS5x6IC5Ha"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the dataset from file\n",
        "with open('dataset.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "# Tokenization\n",
        "tokens = nltk.word_tokenize(text)"
      ],
      "metadata": {
        "id": "b3PJ03h3C8EZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercasing\n",
        "tokens = [token.lower() for token in tokens]"
      ],
      "metadata": {
        "id": "Lx8XxnYsDdyf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing punctuation and special characters\n",
        "tokens = [re.sub(r'\\W+', '', token) for token in tokens if re.sub(r'\\W+', '', token)]\n",
        "\n",
        "# Removing stop words (optional)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Add a special token for unknown words\n",
        "tokens.append('')\n"
      ],
      "metadata": {
        "id": "YhulA3ftD8Yf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numericalization\n",
        "vocab = list(set(tokens))\n",
        "word2index = {word: i for i, word in enumerate(vocab)}\n",
        "index2word = {i: word for i, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "q3cI0CPfD_jH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating sequences\n",
        "sequence_length = 5\n",
        "sequences = []\n",
        "for i in range(len(tokens) - sequence_length):\n",
        "    sequences.append(tokens[i:i + sequence_length])\n"
      ],
      "metadata": {
        "id": "InUVhr_3ECca"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sequences to numerical indices\n",
        "input_sequences = []\n",
        "for sequence in sequences:\n",
        "    input_sequences.append([word2index[word] for word in sequence])\n"
      ],
      "metadata": {
        "id": "3wO44hYUEFwJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to numpy array\n",
        "input_sequences = np.array(input_sequences)\n",
        "\n",
        "print(f\"Total sequences: {len(input_sequences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEAv4VY6EHzu",
        "outputId": "72bab17d-9c83-4c25-a02e-f7a22fdf1cad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sequences: 21210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        x = self.embedding(x)\n",
        "        x, state = self.lstm(x, prev_state)\n",
        "        x = self.fc(x)\n",
        "        return x, state\n",
        "\n",
        "    def init_state(self, batch_size=1):\n",
        "        return (torch.zeros(2, batch_size, self.lstm.hidden_size),\n",
        "                torch.zeros(2, batch_size, self.lstm.hidden_size))"
      ],
      "metadata": {
        "id": "aHivOzO7EKqi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 50\n",
        "hidden_dim = 100\n",
        "vocab_size = len(vocab)\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "# Model, loss function, optimizer\n",
        "model = LanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, input_sequences, criterion, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for i in range(0, len(input_sequences) - batch_size, batch_size):\n",
        "            inputs = torch.tensor(input_sequences[i:i + batch_size, :-1], dtype=torch.long)\n",
        "            targets = torch.tensor(input_sequences[i:i + batch_size, 1:], dtype=torch.long)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            state_h, state_c = model.init_state(batch_size)\n",
        "            outputs, _ = model(inputs, (state_h, state_c))\n",
        "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / (len(input_sequences) // batch_size)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, input_sequences, criterion, optimizer, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Kxoh_LtEN98",
        "outputId": "adc6f9e5-82f3-4c91-df8d-9b401f54d7ae"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 6.6853\n",
            "Epoch 2/20, Loss: 5.1809\n",
            "Epoch 3/20, Loss: 4.5664\n",
            "Epoch 4/20, Loss: 4.0004\n",
            "Epoch 5/20, Loss: 3.4469\n",
            "Epoch 6/20, Loss: 2.9702\n",
            "Epoch 7/20, Loss: 2.5734\n",
            "Epoch 8/20, Loss: 2.2383\n",
            "Epoch 9/20, Loss: 1.9564\n",
            "Epoch 10/20, Loss: 1.7181\n",
            "Epoch 11/20, Loss: 1.5147\n",
            "Epoch 12/20, Loss: 1.3463\n",
            "Epoch 13/20, Loss: 1.2080\n",
            "Epoch 14/20, Loss: 1.0970\n",
            "Epoch 15/20, Loss: 1.0062\n",
            "Epoch 16/20, Loss: 0.9317\n",
            "Epoch 17/20, Loss: 0.8736\n",
            "Epoch 18/20, Loss: 0.8266\n",
            "Epoch 19/20, Loss: 0.7885\n",
            "Epoch 20/20, Loss: 0.7582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmrgP32HEdCO",
        "outputId": "ca698a8a-647d-471d-fc7b-742c29366eb9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_text, max_length=50):\n",
        "    model.eval()\n",
        "    words = start_text.split()\n",
        "    state_h, state_c = model.init_state(batch_size=1)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Use the word2index dictionary with a fallback to  if the word is not found\n",
        "        x = torch.tensor([[word2index.get(w, word2index['']) for w in words]], dtype=torch.long)\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(index2word[word_index])\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Test the model with a sample input\n",
        "start_text = \"harry potter is\"\n",
        "generated_text = generate_text(model, start_text)\n",
        "print(\"Generated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOC-6IQAEd_I",
        "outputId": "35317978-07a3-4ea8-a730-5de9714a3622"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "harry potter is roundedfull textsm bordergray100 transitiontransform grouphover textgray400 dark fillgray50080 rx 2 x 528 25 width 112 height 4 fillopacity 1 rect rect class fillindigo500 cursorpointer x rounded path path class originbottomright transitiontransform grouphover rotate6 60539 ranging fear shame hope god love rzayev 0566711 professional expand 30353 2725 68042 331486 dark original\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X0j3hkvEh9O",
        "outputId": "3bba66bc-fb56-4094-ef2f-c528a7d50254"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.23.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.41.1 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "# Define the model class (this should match your trained model's class)\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        x = self.embedding(x)\n",
        "        x, state = self.lstm(x, prev_state)\n",
        "        x = self.fc(x)\n",
        "        return x, state\n",
        "\n",
        "    def init_state(self, batch_size=1):\n",
        "        return (torch.zeros(2, batch_size, self.lstm.hidden_size),\n",
        "                torch.zeros(2, batch_size, self.lstm.hidden_size))\n",
        "# Load the trained model\n",
        "model = LanguageModel(vocab_size=len(vocab), embedding_dim=50, hidden_dim=100)\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Define the text generation function\n",
        "def generate_text(model, start_text, max_length=50):\n",
        "    model.eval()\n",
        "    words = start_text.split()\n",
        "    state_h, state_c = model.init_state(batch_size=1)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Use the word2index dictionary with a fallback to  if the word is not found\n",
        "        x = torch.tensor([[word2index.get(w, word2index['']) for w in words]], dtype=torch.long)\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(index2word[word_index])\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Streamlit app interface\n",
        "st.title(\"Text Generation with Language Model\")\n",
        "st.write(\"Enter a text prompt and the model will generate a continuation of the text.\")\n",
        "\n",
        "# Input box for user to type in a text prompt\n",
        "user_input = st.text_input(\"Enter text prompt:\", \"Harry Potter is\")\n",
        "\n",
        "# Generate text based on user input\n",
        "if user_input:\n",
        "    with st.spinner('Generating text...'):\n",
        "        generated_text = generate_text(model, user_input)\n",
        "        st.success(\"Generated Text:\")\n",
        "        st.write(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azy55oz-Eq-D",
        "outputId": "b5a4528b-a385-43c3-b6fb-f3d24f33b311"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-9a650539a095>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('model.pth'))\n",
            "2025-01-26 14:49:46.584 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.666 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-01-26 14:49:46.668 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.670 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.672 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.674 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.675 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.677 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.678 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.679 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.680 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.681 Session state does not function when running a script without `streamlit run`\n",
            "2025-01-26 14:49:46.682 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.683 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.684 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.685 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.686 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.795 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.796 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.800 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.801 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.807 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.809 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-01-26 14:49:46.810 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    }
  ]
}